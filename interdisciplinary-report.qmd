---
title: "Applying Detection of Eye Movement With Physics and Machine Learning"
author:
  - "510517588"
  - "510462200"
  - "500465031"
  - "500411142"
  - "500443660"
  - "490021440"

format: html
toc: true
number-headings: true
embed-resources: true
code-fold: true
bibliography: interdisciplinary-report.bib
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(reticulate)
library(knitr)
```

```{python import-things, echo=FALSE, message=FALSE, warning=FALSE}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import ScalarFormatter
import seaborn as sns
import os
from scipy import signal

from models import top_models, top_features
from constants import *
from read_model_performance import get_model_performance

# loads cached results and times for all models
model_performance = get_model_performance()

sns.set_palette(['#323A4C', '#C65861'])
```

# Executive Summary

The core challenge of this project was to develop an intuitive and user-friendly product tailored to the needs of individuals with disabilities. We addressed this by creating an eye-tracking controlled RC car, utilising machine learning and data science techniques to optimise the accuracy, latency and robustness of our product.

From a physics approach, we applied critical thinking in evaluating different electrode positions, and filtering techniques such as Fast Fourier Transform and downsampling. Furthermore, through analysis of data patterns of maxima and minima, we were able to create an automatic event interval. 

The results demonstrated an average software latency of 0.01 seconds in classification, false positive rate of 0.4%, and less than a 5% difference in accuracy between trained and untrained users.

Our eye movement detection technology holds immense potential in creating more accessible products for individuals with disabilities where hands-free technology is required.

# Introduction

In our world, people with disabilities face challenges due to their lack of autonomy, impacting their social and occupational lives. We aim to develop a fast, reliable eye-movement detection technology, combining Physics and Data Science, and initially tested via an RC car. This technology will enhance an inclusive play experience for the disabled. Our versatile product could extend to healthcare for wheelchair control and cognitive testing. We'll investigate how different factors and processes affect Electro-Oculogram Activity (EOG) signal quality, and explore Machine Learning techniques for effective eye movement classification, considering the relationship between concepts such as Nyquist’s theorem, down-sampling, and signal input variations.

# Methods

![Project Design Workflow](img/figure-01.png){#fig-01}

We developed an iterative process, in which we tested different data collection/cleaning methods, as well as different classification methods and variables. Considerations combining Physics and Data Science were made at every step in the process, including the most optimal way to collect signals, filter signals, classify events, and eventually predict events. The model optimisation process was completed for each different classifier method (see @fig-classifier-eval)

# Data Collection

The scale and reliability of data collected underpins the quality of event classification. We collected data containing sequences of labelled eye movements (left-look, right-look, and blink), so we could build a classification model which accurately predicts these movements. We saved these as `.csv` files, each associated with another file containing the "markers" (labels and times) of each eye movement. In total, we recorded 813 events. See the appendix for definition and notation of events.

## Electrode Position

Electrode position is crucial in obtaining distinguishable signals that will correspond to actions by the RC car. Initially, electrodes were placed between the eyebrows and on the upper temple/eyebrow area, forming a horizontal line. This position was adequate for classification of left and right looks, however there were concerns about the implementation of a clear blinking signal, as this position was inconsistent. 

We explored the relationship between electrode placement and the signal strength, by varying each electrode position and calculating the signal strength through the average power spectral density (PSD) as a function of distance from the left eye (see appendix). These results indicate that electrode placement should be as close as possible to the eyes. We can see the effect of this in @fig-electrode-signal, where the placement of the electrode forms a horizontal line that intersects the eyes (see @fig-electrode-placement).

```{python fig-electrode-signal, fig.cap="Filtered Electrode Signal Using Different Electrode Placement", fig.width=4, message=FALSE, warning=FALSE, results=FALSE}
def plot_features(df, ax):
    '''
    Plots event intervals for signals given a dataframe
    '''
    for i in df[df['event'] == 1]['T']:
        ax.axvline(i, color='g',label='Left')

    for i in df[df['event'] == 2]['T']:
        ax.axvline(i, color='g', linestyle='--')

    for i in df[df['event'] == 3]['T']:
        ax.axvline(i, color='r', label='Right')
    for i in df[df['event'] == 4]['T']:
        ax.axvline(i, color='r', linestyle='--')

    for i in df[df['event'] == 13]['T']:
        ax.axvline(i, color='orange', label='Blink')
    for i in df[df['event'] == 14]['T']:
        ax.axvline(i, color='orange', linestyle='--')
    return None


fig, (ax1, ax2) = plt.subplots(1,2)

#weak data path
weak_data = f'Test_csv/CL/FFT/101FFT'
#read in dataframe from csv
df = pd.read_csv(f'{weak_data}.csv')

#plot signal and event intervals
ax1.plot(df['T'], df['V'])
plot_features(df, ax1)
ax1.set_xlabel('time (s)')
ax1.set_title('Original Electrode Position')
ax1.set_ylabel('Voltage (a.u)')
ax1.legend()
ax1.set_ylim(-50, 1050)

#FFT data path
FFT_data = f'Test_csv/CL/FFT/1FFT'

#read in dataframe from csv
df = pd.read_csv(f'{FFT_data}.csv')

#plot signals and event intervals
ax2.plot(df['T'], df['V'])
plot_features(df, ax2)
ax2.set_xlabel('time (s)')
ax2.set_title('Optimised Electrode Position')
ax2.set_ylabel("Voltage (a.u.)")
ax2.legend()
ax2.set_ylim(-50, 1050)

fig.set_figwidth(10)

plt.show()
```

## Automatic Markers

Originally, we labelled events using a Python script, which marked events based on a timed keypress, however there was a steep learning curve for accurately labelling events. Improving this, we implemented an automatic marker based on observations from previous data, which suggested the start of an event was approximately 0.15 seconds before maximum/minimum voltage. All events were considered to have a length of 0.5 seconds. The implementation of automatic markers greatly increased the scale and reliability of our training data.

# Data Cleaning/Transformation

The required input for the classification models is a list of aggregated features from the window being classified. The training data was created by splitting signal sequences into overlapping 1-second windows, which were then cleaned, and features were extracted. This provides the classifiers better insight into the signals in each window, without having the entire sequence being input as attributes to the classifier.

## Fast Fourier Transform

A degree of noise is captured in the components of the measuring equipment, such as the wires, electrodes, and internal working of the SpikerBox. We can implement Fast Fourier Transforms to convert signals from the time domain into the frequency domain which enables observing what frequencies compose our EOG signal. This enables the use of a ‘Gaussian blur’ which acts as a low-pass filter that blocks noisy high frequency components. This is effective since the frequency of our desired signals are less than 5Hz. This plays a major role in producing clean signals for Data science to perform machine learning.

## Rolling Windows

From each sequence in the collected data, overlapping 1-second windows, each 0.1 seconds apart, were extracted (see @fig-rolling-window). This increased the scale of the training data, as opposed to fully-separated windows, thus increasing our classification models' accuracy. This also better simulates the streaming condition, since we are performing classifications every 0.1 seconds, so we have to be able to detect eye-movements which are located in different portions of each window. This increases the product's reliability.

## Feature Set

The feature set was decided by examining the waveforms of each eye movement (see @fig-feature-set), and deciding how to capture event waveforms, using the fewest attributes. Signal variation between users was accounted for by including features independent of baseline values (i.e., Difference between median and maximum values). The reduced feature set reduces the classification models' complexity and latency, when compared to alternatives such as TSFresh, where there are 770+ features, which aren't tailored towards the type of the data. Further improvements could be made by considering a larger set of features, or combining our features with TSFresh.

# Machine Learning Model

We used a Machine Learning model to classify every window collected from the SpikerBox, as one of:

1. Non-event
2. Left
3. Right
4. Blink

We considered using a simple threshold (i.e., standard deviation) for event detection, however these measures weren't robust, as optimal thresholds varied between users and environments. For example, suboptimal electrode placement increased the signal noise. To solve this, we included Non-events as a target class, and the Machine Learning model better captured the waveforms’ complexity, particularly due to the feature set explored earlier.

## Classifier Evaluation

### Workflow

We tested many combinations of models, hyperparameters, downsampling rates, and feature selection methods. To test each of these, we ran 50-times 5-fold cross validation, saving various performance metrics each iteration. The cross-validation prevented overfitting, and repetitions minimised the effect of outliers, providing a more precise measurement of metrics.

During the evaluation phase, the accuracy was initially massively inflated, because one of the features is the classification of the previous window, and the training data had 100% accurate labels for this feature. This meant that the accuracy relied on the assumption that the model always classified the previous window correctly. To overcome this, we stored the windows' original sequence and position in that sequence. Then, the training data was split by sequence, instead of window. The testing was done by classifying one sequence (in order) at a time, and using the previous prediction as the label for the corresponding feature in the next window. This meant that the testing was done in the same condition as in real product use, providing more representative accuracy measures.

The software latency of each model was calculated by simulating 100 random windows, and measuring the time for each window's filtering, feature extraction, and classification. This allows us to see how long it takes from signal input to classification output.

### Metrics

We used the proportion of correctly labelled windows as a basic metric to gauge the relative performance of different models when making early design decisions. We did this by dividing the number of correctly classified windows, by the number of total windows. Although this doesn't give a good idea of how well the product works in terms of real usage, it is strongly linked to event accuracy and false positive rate, and thus worked as a good proxy during early development.

When making larger decisions, more closely linked to the end product, we evaluated the false positive rate. This was done by dividing the number of times an "event" was detected, when no event occurred, by the total number of actual events. This measure is important in ensuring the safety of resulting products, since false positives would result in unwanted movement, which poses a safety risk.

We also used event accuracy, measuring the number of misclassified events, compared to the number of total events. This is a better metric for product accuracy than the window accuracy, since it relates directly to the feel of the product.

Our software latency metric measured the average time from signal input to classification, and allowed us to adjust the SpikerBox's buffer size according to the speed. Our goal was to get the average classification time under 0.1 seconds, since this is the minimum buffer size, and is also the speed required for technology to feel seemingly instant [@response-time].

## Model Selection

### Hyperparameters and Variables

The hyperparameters and variables for each classification method (see @fig-classifier-eval for methods) were selected by evaluating all variations of them, tested on training data with no downsampling, and using the full feature set. Then, only the models with the highest accuracy for each method were kept. Keeping all other variables constant allowed for fair comparison, however it would've been ideal to test each of these models on various different feature selection methods and downsampling rates, since some of the models may have performed better in alternate circumstances. Unfortunately, this is not viable, since the model evaluations' computation time is too large.

### Downsampling

From physics, Nyquist’s theorem states that we can capture all important features of our signal by sampling points at a rate double the maximum frequency we want to keep. By inspecting the signals’ periodograms (see @fig-downsampling), we can see that major signal strength lies in regions below 5Hz. This means we can obtain a theoretical downsampling rate of approximately 1000.

To validate this prediction and find the optimal downsample rate, we evaluated the window classification accuracy, and the latency, of a fixed set of models, under various different downsampling rates. @fig-downsampling confirms that a downsampling rate of 1000 is viable, however a rate of 200 was found to provide the best tradeoff between accuracy and classification time. Evidently, communication between disciplines was essential to produce this result.

```{python fig-downsampling, fig.cap="Impact of Downsampling on Signals and Classification", fig.width=4, results=FALSE, warning=FALSE}
# Frequency plot
path = f'{os.getcwd()}/Compare_PSD_Left/FORWARDS/'
df = pd.read_csv(f'{path}/CL/FFT/3cm3FFT.csv')

index = df[df['event'] == 1].index[0]
index2 = df[df['event'] == 2].index[0]

f, Pxx_den = signal.periodogram(df['V'].iloc[index:index2], 10000)

fig, (ax1, ax2) = plt.subplots(1, 2)
ax1.semilogx(f[f<100], Pxx_den[f<100])
ax1.axvline(5, color= 'red', linestyle='--', label='5Hz')
ax1.set_ylabel('PSD [V**2/Hz]')
ax1.set_xlabel('Frequency (Hz)')
ax1.set_title('Power in frequency components of CL signal')
ax1.legend()

# Downsampling plot
relevant_models = {model: value for model, value in model_performance.items() if model[0]
                    in top_models and model[2] == 'none'}
times = {x: list() for x in DOWN_SAMPLE_RATES}
accuracies = {x: list() for x in DOWN_SAMPLE_RATES}

for model, performance in relevant_models.items():
    times[model[1]].append(performance['time'])
    accuracies[model[1]].append(performance['accuracy'])

mean_times = [np.mean(values) for _, values in times.items()]
mean_accuracies = [np.mean(values) for _, values in accuracies.items()]
accuracy_bins = list()
for accuracy in mean_accuracies:
    if accuracy > 0.87:
        accuracy_bins.append('> 0.87')
    elif accuracy > 0.85:
        accuracy_bins.append('> 0.85')
    else:
        accuracy_bins.append('<= 0.85')

df = pd.DataFrame({
    'Down Sample Rate': DOWN_SAMPLE_RATES,
    'Average Classification Time (s)': mean_times,
    'Average Accuracy': accuracy_bins
})

green = "#008001"
yellow = "#FFA500"
red = "#FF3128"

sns.scatterplot(df,
                x='Down Sample Rate',
                y='Average Classification Time (s)',
                hue='Average Accuracy',
                label='Average Accuracy',
                palette=sns.color_palette([green, yellow, red]),
                marker='o',
                zorder=1)
sns.lineplot(df,
              x='Down Sample Rate',
              y='Average Classification Time (s)',
              color='#323A4C',
              linewidth=1,
              alpha=0.5,
              zorder=0)

# set x-axis label
ax2.set_xlabel('Down Sample Rate', fontsize=14)
# set y-axis label
ax2.set_ylabel('Average Classification Time (s)',
              fontsize=14)
ax2.set_title('Classification Time vs. Down Sample Rate')
ax2.set_xscale('log')

for axis in [ax2.xaxis, ax2.yaxis]:
    axis.set_major_formatter(ScalarFormatter())

fig.set_figwidth(10)

plt.show()
```


### Feature Selection

The feature selection method was decided by evaluating the latency and window accuracy of the best of each classification method using different feature selection methods, using a downsampling rate of 200. Then, the method with the best accuracy for each model was selected. Latency was ignored, because it is well below our target regardless of feature selection.

### Classifier Method

To decide on the final classifier, we evaluated the event accuracy and false positive rate of each classifier method, using their best feature selection methods. The K-Nearest Neighbours and Random Forest classifiers had the best results, and we ended up choosing the Random Forest classifier, due to the lower false positive rate (see @fig-best-model). This is because we felt that user safety was a higher priority than ease of use.

# Resulting Product

The RC car was designed to have bluetooth connection via an Arduino. The components are: 

- SpikerBox
- Electrode stickers
- Alligator clips
- Car body
- HC-05 bluetooth
- Arduino UNO R3 
- 2 12V motors
- L298N Motor controller
- 2 9V battery sources for Arduino and motor controller.

The design of the signals used on the car were to be intuitive, with spin left/right for directional looks and to stop spinning when eyes returned to forward position and a fast blink to move forward and stop. The blinking motion needs to be emphasised in order to not classify a normal blink as forward motion. The action of a fast blink also means that the user’s eyes are able to be fixed on the car before it starts moving forward.

![Electrode Placement (Left) and Car Setup (Right)](img/product-setup.png){#fig-product-setup}

## Deployment

Once the user is set up with the electrode stickers attached to the SpikerBox, a random Forest classifier is trained on the training data, with a downsampling rate of 200, and Recursive Feature Elimination (using Logistic Regression) applied. Then, once the streaming begins, users’ eye movements will be picked up as signals in the electrode stickers. These signals go to the SpikerBox, which sends 1000 signals every 0.1 seconds to a computer. There, the most recent 1 second of data is filtered, downsampled, and aggregated to the features in @fig-selected-features. These features are given as an input to the classifier, which outputs a classification for that window. If there is a new event, a bluetooth signal is then sent out to the RC car, which interprets the events and controls the movement using DC motors.

Some practical considerations for obtaining the cleanest signals for optimal classification performance:

- **Clean skin** - Removing dirt and oil will enhance the signal.
- **Laptop Chargers** - Laptop chargers should be disconnected to not cause any electrical interference.
- **Check battery** -  SpikerBox requires a power source above 7.5V.

## Product Performance

```{python fig-results, echo=FALSE, fig.cap="Final Classifier Results", warning=FALSE, fig.width=4}
top_model_results = pd.read_csv('results/best_model_results.csv')

fig, (ax1, ax2) = plt.subplots(1,2)

# Comparing event accuracy across environments
sns.boxplot(data=top_model_results,
            ax=ax1,
            x='Metric',
            y='Value',
            linewidth=0.5)

ax1.set_xlabel('Environment')
ax1.set_ylabel('Event Accuracy')
ax1.set_title('Comparing Event Accuracy between Environments')

# Looking at latency
times = pd.read_csv(f'results/real_latency.csv')

sns.boxplot(data = times,
            ax=ax2,
            y = 'Latency')

ax2.set_ylabel('Hardware Latency (seconds)')
ax2.set_title('Distribution of Hardware Latency')

fig.set_figwidth(10)
plt.show()
```

```{python}
cv_acc = 100 * np.round(top_model_results.groupby(["Metric"])["Value"].mean()['Event Accuracy (CV)'], 3)
lab_acc = 100 * np.round(top_model_results.groupby(["Metric"])["Value"].mean()['Event Accuracy (Lab)'], 3)

top_model = model_performance[('Random Forest #1:', 200, 'RFE_LR')]
cv_fpr = 100 * np.round(top_model['false_positives'] / top_model['num_events'], 3)

soft_latency = np.round(top_model['time'], 2)
mean_latency = np.round(np.mean(times['Latency']), 2)
```

### Classification Accuracy

The final accuracy for classification of events (evaluated using cross-validation) was `r py$cv_acc`%. We also tested the event accuracy by using the final product, and recording whether each event was correctly classified. This was done across multiple users, some of whom did not contribute to the training data. This accuracy was `r py$lab_acc`%.

### False Positive Rate

We measured the false positive rate using cross-validation, and found that our classifier had only a `r py$cv_fpr`% false positive rate. We did not formally measure the false positive rate in real use of the product, however no false positives occurred during all use of the product.

### Latency

Our product has an average software latency of `r py$soft_latency` seconds. We also measured the latency of the entire product, by recording videos of eye movements and corresponding car movements, and recording the time between them. Using this method, we found that the product's latency is `r py$mean_latency` seconds on average, although sometimes was as large as 1 second.

### Robustness

```{python}
string = '10111111111101111110111111111101111101111011101111110100111101011000110111101011110111101001111010110110'
ls = list(string)
untrained_blink = 100 * np.round(sum([i == '1' for i in ls])/len(ls), 3)

string = '1111011011110111011111111111101101111011101111111111011101101101011111011111011111101110110110101111101111101111'
ls = list(string)
trained_blink = 100 * np.round(sum([i == '1' for i in ls])/len(ls), 3)

string = '1111111111011111111110111011101111111111111111111111111011111111111011101111111111111111111111111011111111'
ls = list(string)
untrained_direction = 100 * np.round(sum([i == '1' for i in ls])/len(ls), 3)

string = '11111111111111111110111111111111111111110111111110111111111111111111111111111111101111111111111111111101111111101111111111111111111111'
ls = list(string)
trained_direction = 100 * np.round(sum([i == '1' for i in ls])/len(ls), 3)
```

We measured robustness by comparing different users' event classification accuracy, under lab conditions. We found that there was less than a 5% difference in real accuracy between users who trained the data, and users who didn't. For trained users, directional accuracy was `r py$trained_direction`% and blinking accuracy was `r py$trained_blink`%. For untrained, directional accuracy was `r py$untrained_direction`% and blinking accuracy was `r py$untrained_blink`%.

### User Experience

After extensive product use, we have found that there is a large learning curve for controlling the car. This includes learning exactly what type of eye movement is expected by the classifiers, particularly for blinking. Furthermore, turning was difficult at times, since users couldn't look at the car while turning.

Directional movements were too fast, making it difficult to turn less than 180 degrees. Also, sometimes bluetooth output was missed by the Arduino, leading to correctly classified events being “missed”.

Accuracy slightly decreased compared to lab testing. Factors like nerves, SpikerBox battery state, distraction levels, and variable electrode placement affected classification performance. The classifier was ineffective when improperly set up but performed well under optimal conditions.

# Discussion

Our performance measures varied significantly across cross-validation, lab-condition testing, and product testing. This is because the training data was collected in a controlled environment with optimal electrode placement, battery health, and minimal distractions. These factors affect signal magnitude and noise. The relative movement between the centre and left changes in different environments, impacting the maximum signal value. As a result, our cross-validation metrics assume controlled variables, which do not reflect real user experience. Additionally, many sequences were collected in the same session, which was not accounted for in our cross-validation splits, leading to inflated performance. To address this, we will separate training data by session in the future and collect data in suboptimal conditions to capture real-world variability in the classification model.

Signal output from blinks varied between users. A “fast blink” is subjective, so new users will need to practise to get this correct. We explored the use of other alternatives such as winking however this led to even more variability in signals, since not everyone can wink easily.

The overall latency was suboptimal, due to hardware limitations. Upgrading hardware can reduce this in future. Considering SpikerBox buffering, the real latency isn’t captured. If an event occurs at the beginning of the buffer, the classifier receives this information 0.1 seconds later. Consequently, the software’s true latency is 0.07 seconds on average, since the location of an event in the buffer will be in the middle, on average. 

Our system is fairly robust due to cross-validation, feature independence from individual baselines, and contributions from four trainers.

# Conclusion

## Findings

We achieved our aim in creating fast, reliable eye-movement detection technology to create ocular-enabled manoeuvrability. Our classifier had excellent performance metrics including a real accuracy rate of `r py$lab_acc`%, classification latency of `r py$soft_latency` seconds, and less than 5% difference in accuracy between trained and untrained users. While we did face some limitations, including variability in user interpretation of signals and some hardware constraints, these challenges provided insights for future enhancements.

## Future Work

To improve our technology, we can explore improvements such as semi-supervised machine learning algorithms, and building a more robust, diverse dataset, accounting for external factors’ influence on signal features.

This project's ability to detect eye movements to allow ocular-enabled manoeuvrability can be extended to several industries. We could explore the use of this technology for controlling wheelchairs, as well as using the eye movement detection for health and accessibility purposes, such as controlling software, or completing cognitive tests for the physically disabled.

# Appendix

## Additional Information

### Defining Eye Movement "Events"

Our classification method detects the beginning and end of directional eye movements as separate events, allowing us to measure the duration of a left or right look. This was designed to allow for more control over the RC car. We denote an event as a string: “C” for when the user is looking forward, “L” for left, “R” for right, and “B” for a fast blink. For example, “CLCB” denotes the user looking forward, then left, then back to forward, and finally blinking. In this project we fixed the left signal to produce a maximum.

### Further Electrode Placement Analysis
Testing involved moving the left temple electrode down the face, while keeping the central electrode fixed.Then the central eyebrow electrode was moved vertically up the face while the left electrode was fixed. Each data point was an average of 3 of each directional event.

@fig-moving-electrodes indicates that the left electrode follows an inversely proportional relationship , but the central electrode remains roughly constant. This means that to maximise the left and right signals, we should move the electrodes closer to the eye such that the lines connecting the electrodes are horizontal and intersects the eyes. Furthermore, this should act to provide complementary signals, as the left and right eye have symmetric electrical potentials and thus left and right looks would have distinguishing signals. @fig-electrode-signal confirms this analysis as we attain stronger signals that make it easier to distinguish blinks.

## Additional Figures

![Rolling Window Representation](img/rolling-window.png){#fig-rolling-window}

![Electrode Placement: Blue - Original Position, Red - Optimal Position](img/electrode-placement.png){#fig-electrode-placement}

![Classifier Construction Workflow](img/classifier-workflow.png){#fig-classifier-eval}

```{r fig-feature-set, fig.cap="Full Feature Set"}
feature_set = matrix(c("Minimum value",
                "Maximum value",
                "Mean",
                "Standard deviation",
                "Median",
                "Difference between maximum value and median",
                "Difference between minimum value and median",
                "Proportion of values larger than the mean",
                "Index of minimum value",
                "Index of maximum value",
                "Crossings over the median",
                "Proportion of increasing values",
                "Previous window's classification",
                ""),
                ncol=2)
kable(feature_set)
```

```{python fig-best-model, fig.cap="Comparison of Classification Algorithms"}
relevant_models = {k: v for k,
                   v in model_performance.items() if k[0] in top_models and k[1] == DOWN_SAMPLE_RATE and k[2] == top_features[k[0]]}
model_names = [model[0].split(' #')[0] for model in relevant_models]
false_positives = [model['false_positives']
                   for _, model in relevant_models.items()]
misclassifications = [model['misclassifications']
                      for _, model in relevant_models.items()]
num_events = [model['num_events'] for _, model in relevant_models.items()]

data = pd.concat([pd.DataFrame({
    'Model': model_names,
    'Metric': ['False Positive Rate' for _ in range(len(model_names))],
    'Value': [false_positives[i] / num_events[i] for i in range(len(false_positives))]
}),
    pd.DataFrame({
        'Model': model_names,
        'Metric': ['Misclassification Rate' for _ in range(len(model_names))],
        'Value': [misclassifications[i] / num_events[i] for i in range(len(misclassifications))]
    })])

plot = sns.catplot(data,
            kind='bar',
            x='Model',
            y='Value',
            hue='Metric',
            palette=sns.color_palette(['#D94552', '#124F7B']))
plt.title("Comparison of Classification Algorithms")
plt.subplots_adjust(left=0.09, top=0.92, right=0.95)
plt.show()
```


```{r fig-selected-features, fig.cap="Features Selected by Recursive Feature Elimination (using Logistic Regression)"}
selected_features = matrix(c("Standard deviation",
                      "Difference between maximum value and median",
                      "Difference between minimum value and median",
                      "Index of minimum value",
                      "Index of maximum value",
                      "Previous window's classification"))
kable(selected_features)
```

```{python fig-moving-electrodes, fig.cap="Measuring Signal Quality at Different Electrode Positions"}
######################

path = f'{os.getcwd()}/Compare_PSD_Left/FORWARDS/'
dict = {}
for folder in os.listdir(path):
    for file in os.listdir(f'{path}/{folder}/FFT'):
        string = file.split('cm')

        df = pd.read_csv(f'{path}/{folder}/FFT/{file}')
        if folder == 'CL':
            index = df[df['event'] == 1].index[0]
            index2 = df[df['event'] == 2].index[0]
        elif folder == 'CR':
            index = df[df['event'] == 3].index[0]
            index2 = df[df['event'] == 4].index[0]

        f, Pxx_den = signal.periodogram(df['V'].iloc[index:index2], 10000)

        if string[0] not in dict:
            dict[string[0]] = []
        
        dict[string[0]].append(np.mean(Pxx_den))


ls = []
for i in dict:
    dict[i] = np.mean(dict[i])

del dict['4']
dict['11'] = dict.pop('11')
plt.plot(list(map(int, dict.keys())), dict.values())

##########################################################################

path = f'{os.getcwd()}/Compare_PSD_Central/FORWARDS/'
dict = {}
for folder in os.listdir(path):
    for file in os.listdir(f'{path}/{folder}/FFT'):
        string = file.split('cm')

        df = pd.read_csv(f'{path}/{folder}/FFT/{file}')
        if folder == 'CL':
            index = df[df['event'] == 1].index[0]
            index2 = df[df['event'] == 2].index[0]
        elif folder == 'CR':
            index = df[df['event'] == 3].index[0]
            index2 = df[df['event'] == 4].index[0]

        f, Pxx_den = signal.periodogram(df['V'].iloc[index:index2], 10000)

        if string[0] not in dict:
            dict[string[0]] = []
        
        dict[string[0]].append(np.mean(Pxx_den))


dict['6.5'] = dict.pop('6_5')

ls = []
for i in dict:
    dict[i] = np.mean(dict[i])
dict['8'] = dict.pop('8')
dict['11'] = dict.pop('11')
plt.plot(list(map(float, dict.keys())), dict.values())

plt.title('PSD vs Electrodes Distance from Left Eye')
plt.xlabel('Distance from Left eye (cm)')
plt.ylabel('Average PSD [V**2/Hz]')
plt.legend(['Left', 'Central'])
plt.grid()

plt.show()
```


## Student Contributions

**510462200** - I created Python Script for Automatic Event Markers and data collection. I had the physicist roles of signal analyst of electrode placement, theoretical downsample rate. Responsible for all technical hardware of the project - researching, constructing, coding the Arduino car, SpikerBox. I was in charge of organising data collection procedures and collecting the majority of data. I contributed physics sections to the report and presentation and performed the live demonstration and recorded a video backup. 

**510517588** - I wrote all the code for the streaming classifier, downsampling, feature selection, and classifier evaluation, as well as code for several different classifier methods. I also wrote the code for extracting features and generating the training data. Myself and 510462200 made most of the project's design decisions. I was a major contributor to both the presentation and report, in charge of creating all of the data science-related plots, as well as structuring, editing, and writing a large portion of the data science side of the report. I designed and wrote most of the slides in the presentation, and wrote the majority of the script.

**500465031** - I helped in the initial research for finding the parts for the Arduino car and what would be required to build it, I was also a contributor for both the presentation and the report working on the Introduction, conclusion, future applications of our technology and findings of our project for the report and the same for the presentation, for the presentation i was also involved in structure, design and presenting. I also was involved throughout the project in data collection multiple times, from the initial trial and error and towards the end as well. 

**500411142** - Wrote a part of model evaluation and model selection code. Focusing on making a presentation slide and report. For the presentation slide part about improving visual experience, combining different pages and extract key points of each sentence. For the report part I did the model evaluation and selection part. I am also a part of data collection. Improve the final essay.

**500443660** - I helped with part of the model evaluation and model selection. For the presentation slide section, I added a little bit of revision. For the report, I did the project results for latency, false positive rate and robustness. I also was involved in some data collection, as a tester helped to complete the car backup work and add to the report as a product performance part.

**490021440** - I wrote a part of the model selection code. For the presentation, I wrote the scripts. And for the report, I did the part of product overview, deployment and event classification accuracy.

## Code

The code is stored on [this GitHub repository](https://github.com/jooshford/eye-movement-classifier). There are instructions for each step of the process in the README file.